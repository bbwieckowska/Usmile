% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/1_function_USprep_mdl.R
\name{USprep_mdl}
\alias{USprep_mdl}
\title{Prepare data for model evaluation for various binary classifiers}
\usage{
USprep_mdl(
  model,
  dataset = NULL,
  testing = FALSE,
  calibrate = FALSE,
  calibration_method = "isotonic"
)
}
\arguments{
\item{model}{Model object (supported classes: 'glm', 'randomForest', 'ranger', 'svm', 'xgb.Booster',
'nnet', 'naive_bayes', 'model_fit' from tidymodels)}

\item{dataset}{Dataset used to prepare data for the models.
\itemize{
\item If \code{testing = TRUE}, a test dataset must be provided (required).
\item If \code{testing = FALSE}, the argument can be:
\itemize{
\item \code{NULL} if the model stores its own training data (e.g., \code{glm}, \code{randomForest}),
\item the training dataset if the model does not keep training data internally
(e.g., \code{ranger}, \code{e1071}, \code{xgboost}, \code{nnet}, \code{naive_bayes}, tidymodels).
This allows the function to work consistently with both training and test data.
}
}}

\item{testing}{Whether to prepare test data (TRUE) or training data (FALSE)}

\item{calibrate}{Whether to calibrate probabilities (default: FALSE)}

\item{calibration_method}{Calibration method ("isotonic" or "logistic")}
}
\value{
A list containing:
\itemize{
\item y - vector of observed values (0/1)
\item p - vector of predicted probabilities for positive class
\item n_vars - number of predictor variables in the model
\item var_names - names of predictor variables in the model
}
}
\description{
This function prepares data for evaluating various binary classification models
including Logistic Regression, Random Forest, SVM, XGBoost, Neural Networks, and Naive Bayes.
Supports both training and test data with optional probability calibration.
}
\note{
This function requires the following packages to be installed for specific model types:
\itemize{
\item \code{randomForest} - for randomForest models
\item \code{ranger} - for ranger models
\item \code{e1071} - for SVM models
\item \code{xgboost} - for XGBoost models
\item \code{nnet} - for Neural Network models
\item \code{parsnip}, \code{workflows} - for tidymodels models
\item \code{naivebayes} - for Naive Bayes models
}
The function will load these packages automatically when needed.
}
\examples{
\dontrun{
# Load required packages
library(tidyverse)
library(randomForest)
library(ranger)
library(e1071)
library(xgboost)
library(nnet)
library(parsnip)
library(workflows)
library(naivebayes)

# Load Heart Disease datasets
data(heart_disease)
data(heart_disease_train)
data(heart_disease_test)

# For Logistic Regression model (glm)

# For logistic regression model (reference model with age, sex, bp, chol)
model_glm_ref <- glm(disease ~ age + sex + bp + chol, data = train_data, family = "binomial")
train_out_glm_ref <- USprep_mdl(model_glm_ref, dataset = NULL, testing = FALSE)
test_out_glm_ref <- USprep_mdl(model_glm_ref, dataset = test_data, testing = TRUE)

# For Random Forest model (randomForest package)
model_rf_ref <- randomForest::randomForest(disease ~ age + sex + bp + chol, data = train_data)
train_out_rf_ref <- USprep_mdl(model_rf_ref, dataset = NULL, testing = FALSE)
test_out_rf_ref <- USprep_mdl(model_rf_ref, dataset = test_data, testing = TRUE)

# For Random Forest model (ranger package)
model_ranger <- ranger(disease ~ age + sex + bp + chol, data = train_data,
                      probability = TRUE, keep.inbag = TRUE)
train_result_ranger <- USprep_mdl(model_ranger, dataset = train_data, testing = FALSE)

# For SVM model (e1071 package)
model_svm <- svm(disease ~ age + sex + bp + chol, data = train_data,
                probability = TRUE, kernel = "radial")
train_result_svm <- USprep_mdl(model_svm, dataset = train_data, testing = FALSE)

# For Neural Network model (nnet package)
train_data_scaled <- train_data
numeric_vars <- c("age", "bp", "chol")
train_data_scaled[numeric_vars] <- scale(train_data[numeric_vars])
model_formula <- disease ~ age + sex + bp + chol
model_matrix_train <- model.matrix(model_formula, data = train_data_scaled)[,-1]
model_nnet <- nnet(x = model_matrix_train,
                  y = as.numeric(train_data_scaled$disease) - 1,
                  size = 10, maxit = 1000, linout = FALSE, entropy = TRUE, trace = TRUE)
model_matrix_with_y <- as.data.frame(model_matrix_pred)
model_matrix_with_y$disease <- as.numeric(train_data_scaled$disease) - 1
train_result_nnet <- USprep_mdl(model_nnet, dataset = model_matrix_with_y, testing = FALSE)

# For Naive Bayes model (naivebayes package)
model_nb <- naive_bayes(disease ~ age + sex + bp + chol, data = train_data)
train_result_nb <- USprep_mdl(model_nb, dataset = train_data, testing = FALSE)

# For tidymodels/parsnip models
model_spec <- logistic_reg() \%>\% set_engine("glm") \%>\% set_mode("classification")
model_tidymodels <- model_spec \%>\% fit(disease ~ age + sex + bp + chol, data = train_data)
train_result_tidy <- USprep_mdl(model_tidymodels, dataset = NULL, testing = FALSE)

# With calibration
model_for_calib <- glm(disease ~ age + sex + bp + chol, data = train_data, family = binomial)
train_result_calibrated <- USprep_mdl(model_for_calib, dataset = NULL, testing = FALSE,
  calibrate = TRUE, calibration_method = "isotonic")
}
}
